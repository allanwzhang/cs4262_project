{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning Model for Melanoma Skin Cancer Classification\n",
    "\n",
    "This notebook implements a transfer learning model using the ResNet50 architecture for the classification of melanoma skin cancer. The model is trained on the Melanoma Skin Cancer Dataset, which consists of 9,600 images for training and 1,000 images for evaluation. ResNet50 is chosen as the base model due to its proven performance in image classification tasks.\n",
    "\n",
    "## Dataset Overview\n",
    "\n",
    "The Melanoma Skin Cancer Dataset contains a total of 10,000 images, with 9,600 images for training and 1,000 images for evaluation. The dataset is designed to facilitate the development of deep learning models for accurate classification of melanoma, which is a deadly form of skin cancer. Early detection and treatment of melanoma can significantly improve patient outcomes and save lives.\n",
    "\n",
    "## Model Architecture\n",
    "\n",
    "The transfer learning model utilizes the ResNet50 architecture as the base model. ResNet50 is a deep convolutional neural network that has been pre-trained on a large-scale image dataset, such as ImageNet. By leveraging the pre-trained weights, we can take advantage of the learned feature representations and adapt them to our specific task of melanoma classification.\n",
    "\n",
    "The pre-trained ResNet50 model is loaded, and its top layers are removed. We then add our own classification layers on top of the base model to perform binary classification (melanoma vs. non-melanoma). The weights of the pre-trained layers are initially frozen to prevent them from being updated during training, allowing us to focus on training the newly added classification layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Import necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation:\n",
    "- Split the datset into the training and validation sets.\n",
    "- Preprocess the images by resizing to a consistent size and normalizing pixel values.\n",
    "- Apply data augmentation techniques to increase diversity of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(root='melanoma_cancer_dataset/train', transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(root='melanoma_cancer_dataset/test', transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Setup:\n",
    "- Load the pre-trained ResNet50 model.\n",
    "- Remove teh top layers of teh pre-trained model.\n",
    "- Add new classification layeres specific to the binary classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityashrey/opt/anaconda3/envs/cs4262/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/adityashrey/opt/anaconda3/envs/cs4262/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/adityashrey/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load pre-trained ResNet50 model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Replace the last fully connected layer\n",
    "num_features = model.fc.in_features\n",
    "model.fc = nn.Linear(num_features, 2)  # 2 classes: benign and malignant\n",
    "\n",
    "# Transfer the model to the device\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training \n",
    "Freeze the weights of the pre-trained model. Train the model on preprocessed training data using binary cross entropy. Monitor the trainign progress and validation performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * images.size(0)\n",
    "    \n",
    "    train_loss = train_loss / len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the trained model on held-out evaluation set. Asses the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/10], Train Loss: 0.1762, Val Loss: 0.2550, Val Accuracy: 0.8990, Val F1 Score: 0.8986\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "val_accuracy = 0.0\n",
    "val_preds = []\n",
    "val_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        val_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        val_accuracy += torch.sum(predicted == labels).item()\n",
    "\n",
    "        val_preds.extend(predicted.cpu().numpy())\n",
    "        val_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "val_loss = val_loss / len(val_dataset)\n",
    "val_accuracy = val_accuracy / len(val_dataset)\n",
    "\n",
    "val_f1 = f1_score(val_labels, val_preds, average='weighted')\n",
    "\n",
    "print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}, Val F1 Score: {val_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4262",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
