{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data transformations\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((100, 100)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "train_dataset = datasets.ImageFolder(root='melanoma_cancer_dataset/train', transform=train_transforms)\n",
    "val_dataset = datasets.ImageFolder(root='melanoma_cancer_dataset/test', transform=val_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of image: torch.Size([3, 100, 100])\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Check the size of the first image and its label in the batch\n",
    "print(f\"Size of image: {images[0].size()}\")\n",
    "print(f\"Label: {labels[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # After three rounds of pooling, size is 100 / 2 / 2 / 2 = 12.5, which rounds down to 12\n",
    "        # Conv3 output is 64 channels by 12x12 spatial size\n",
    "        self.fc1 = nn.Linear(64 * 12 * 12, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        # Flatten the tensor for the fully connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    20] loss: 0.596\n",
      "[1,    40] loss: 0.443\n",
      "[1,    60] loss: 0.427\n",
      "[1,    80] loss: 0.388\n",
      "[1,   100] loss: 0.389\n",
      "[1,   120] loss: 0.314\n",
      "[1,   140] loss: 0.364\n",
      "[1,   160] loss: 0.366\n",
      "[1,   180] loss: 0.355\n",
      "[1,   200] loss: 0.346\n",
      "[1,   220] loss: 0.288\n",
      "[1,   240] loss: 0.296\n",
      "[1,   260] loss: 0.323\n",
      "[1,   280] loss: 0.314\n",
      "[1,   300] loss: 0.276\n",
      "[2,    20] loss: 0.323\n",
      "[2,    40] loss: 0.325\n",
      "[2,    60] loss: 0.289\n",
      "[2,    80] loss: 0.297\n",
      "[2,   100] loss: 0.316\n",
      "[2,   120] loss: 0.347\n",
      "[2,   140] loss: 0.285\n",
      "[2,   160] loss: 0.281\n",
      "[2,   180] loss: 0.263\n",
      "[2,   200] loss: 0.298\n",
      "[2,   220] loss: 0.297\n",
      "[2,   240] loss: 0.276\n",
      "[2,   260] loss: 0.291\n",
      "[2,   280] loss: 0.356\n",
      "[2,   300] loss: 0.275\n",
      "[3,    20] loss: 0.276\n",
      "[3,    40] loss: 0.287\n",
      "[3,    60] loss: 0.272\n",
      "[3,    80] loss: 0.269\n",
      "[3,   100] loss: 0.259\n",
      "[3,   120] loss: 0.270\n",
      "[3,   140] loss: 0.307\n",
      "[3,   160] loss: 0.265\n",
      "[3,   180] loss: 0.291\n",
      "[3,   200] loss: 0.260\n",
      "[3,   220] loss: 0.251\n",
      "[3,   240] loss: 0.234\n",
      "[3,   260] loss: 0.223\n",
      "[3,   280] loss: 0.271\n",
      "[3,   300] loss: 0.264\n",
      "[4,    20] loss: 0.297\n",
      "[4,    40] loss: 0.237\n",
      "[4,    60] loss: 0.248\n",
      "[4,    80] loss: 0.244\n",
      "[4,   100] loss: 0.264\n",
      "[4,   120] loss: 0.273\n",
      "[4,   140] loss: 0.260\n",
      "[4,   160] loss: 0.238\n",
      "[4,   180] loss: 0.274\n",
      "[4,   200] loss: 0.265\n",
      "[4,   220] loss: 0.244\n",
      "[4,   240] loss: 0.280\n",
      "[4,   260] loss: 0.243\n",
      "[4,   280] loss: 0.251\n",
      "[4,   300] loss: 0.231\n",
      "[5,    20] loss: 0.287\n",
      "[5,    40] loss: 0.263\n",
      "[5,    60] loss: 0.228\n",
      "[5,    80] loss: 0.253\n",
      "[5,   100] loss: 0.228\n",
      "[5,   120] loss: 0.252\n",
      "[5,   140] loss: 0.199\n",
      "[5,   160] loss: 0.229\n",
      "[5,   180] loss: 0.273\n",
      "[5,   200] loss: 0.288\n",
      "[5,   220] loss: 0.226\n",
      "[5,   240] loss: 0.230\n",
      "[5,   260] loss: 0.240\n",
      "[5,   280] loss: 0.266\n",
      "[5,   300] loss: 0.239\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 20 == 19:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 20:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the model on the test images: 88.7%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in val_loader:\n",
    "        images, labels = data\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the model on the test images: {100 * correct / total}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4262",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
